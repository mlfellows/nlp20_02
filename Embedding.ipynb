{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline.\n",
    "            1. Word Embedding: using Frequency Base method\n",
    "            \n",
    "            2. Word Embedding: using Prediction Base method.\n",
    "            \n",
    "            3. Word Embedding: using Word2vec & GloVe\n",
    "            \n",
    "            4. Keras Embedding Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Word embedding`** is the collective name for a set of **`language modeling`** and **`feature learning`** techniques in **`Natural language processing (NLP)`** where `words` or `phrases` from the `vocabulary` are *mapped to vectors of real numbers*. \n",
    "\n",
    "Conceptually, it involves a `mathematical embedding` from a space with `many dimensions per word` to a `continuous vector space` with a much `lower dimension`.\n",
    "\n",
    "Methods to generate this mapping include `neural networks`, `dimensionality reduction` on the `co-occurrence matrix`\n",
    "\n",
    "## 1. Word Embedding: using Frequency Base method\n",
    "\n",
    "Includes: `Count Vector`; `tf-idf Vector` and `Co-occurrence Matrix.`\n",
    "\n",
    "### 1.1. CountVectorizer using with `TfidfTransformer` \n",
    "\n",
    "First, consider the simple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 sentences in this corpus\n",
      "The number of the different words is : 11 , and ... they are:\n",
      "['and', 'document', 'first', 'is', 'not', 'one', 'second', 'the', 'third', 'this', 'yours']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document?',\n",
    "          'this Document is not yours..']\n",
    "\n",
    "cvect = CountVectorizer()\n",
    "X = cvect.fit_transform(corpus)\n",
    "\n",
    "print(\"There are %d sentences in this corpus\"%(X.shape[0]))\n",
    "print('The number of the different words is :', X.shape[1], \", and ... they are:\")\n",
    "print(cvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Firstly, they will count how many `different words` in this sentence, here are `11`; noting that both of the words \"`document`\" and \"`Document`\" will be changed to the lower scripts : `\"Document\"`.\n",
    "- Only the second document contains the `word has frequencies = 2`, it is `document`.\n",
    "- The `unique` words in the corpus will be arranged to the `English alphabet characters`; starting at the word \"**a**nd\" and ending by \"**y**ours\".\n",
    "- The `punctuation` (such as `\"?\"` or `\"!\"`, ....) will be ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vocabulary = cvect.get_feature_names()\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary = vocabulary, min_df=2, max_df=0.5, ngram_range=(1,2))),\n",
    "                 ('tfid', TfidfTransformer(smooth_idf=False, use_idf=True))]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **compute the `IDF` values.** (An `idf` is constant per corpus, and **accounts** for the ratio of documents that include the word.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea_names</th>\n",
       "      <th>idf_smooth_False)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>2.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>1.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>1.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not</td>\n",
       "      <td>2.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>2.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>second</td>\n",
       "      <td>2.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>1.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>third</td>\n",
       "      <td>2.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yours</td>\n",
       "      <td>2.609438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fea_names  idf_smooth_False)\n",
       "0        and           2.609438\n",
       "1   document           1.223144\n",
       "2      first           1.916291\n",
       "3         is           1.000000\n",
       "4        not           2.609438\n",
       "5        one           2.609438\n",
       "6     second           2.609438\n",
       "7        the           1.223144\n",
       "8      third           2.609438\n",
       "9       this           1.000000\n",
       "10     yours           2.609438"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame({\"fea_names\": cvect.get_feature_names(), \"idf_smooth_False)\": pipe['tfid'].idf_})\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L987-L992 and https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html?highlight=tfidf#sklearn.feature_extraction.text.TfidfTransformer\n",
    "\n",
    "- If `smooth_idf=False`); the formula that is used to compute the `tf-idf` for a term t of a document `d` in a document set is \n",
    "\n",
    "                                    tf-idf(w, d) = tf(w, d) * idf(w), \n",
    "\n",
    "and the `idf` is computed as \n",
    "\n",
    "                                        idf(w) = log [ n / df(w) ] + 1, \n",
    "\n",
    "where `n` is the `total number of documents in the corpus` and `df(t) is the document frequency of w`; the document frequency is the number of documents in the document set that contain the word `w`. \n",
    "\n",
    "The effect of adding `“1”` to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, ***will not be entirely ignored***. \n",
    "\n",
    "For example: \n",
    "1. The word `\"can\"`. We have a corpus of `5 sentences/ documents` and all of them contain this word (`\"is\"`); so \n",
    "\n",
    "                                    idf(\"is\") = log(5 / 5) + 1 = 1\n",
    "\n",
    "2. The word `\"and\"`, we have\n",
    "\n",
    "                                    idf(\"and\") = log(5 / 1) + 1 appox 2.609 \n",
    "                                \n",
    "Noting that, the `log` here is `natural logarithm (default)`.\n",
    "\n",
    "***Note that the `idf` formula above differs from the standard textbook notation that defines the idf as***\n",
    "\n",
    "                                    idf(w) = log [ n / (df(w) + 1) ].\n",
    "\n",
    "- If `smooth_idf=True` (the default), the constant `“1”` is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which ***prevents zero divisions:*** \n",
    "\n",
    "                                    idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea_names</th>\n",
       "      <th>idf_smooth_False)</th>\n",
       "      <th>idf_smooth_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.182322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>second</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.182322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>third</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yours</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fea_names  idf_smooth_False)  idf_smooth_True\n",
       "0        and           2.609438         2.098612\n",
       "1   document           1.223144         1.182322\n",
       "2      first           1.916291         1.693147\n",
       "3         is           1.000000         1.000000\n",
       "4        not           2.609438         2.098612\n",
       "5        one           2.609438         2.098612\n",
       "6     second           2.609438         2.098612\n",
       "7        the           1.223144         1.182322\n",
       "8      third           2.609438         2.098612\n",
       "9       this           1.000000         1.000000\n",
       "10     yours           2.609438         2.098612"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer(vocabulary = vocabulary, \n",
    "                                           min_df=2, max_df=0.5, ngram_range=(1,2))),\n",
    "                 ('tfid', TfidfTransformer(smooth_idf=True, use_idf=True))]).fit(corpus)\n",
    "\n",
    "table[\"idf_smooth_True\"] = pipe['tfid'].idf_\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.6094379124341005, 2.09861228866811)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## verify the idf_value of the word \"and\"\n",
    "\n",
    "np.log(5) + 1, np.log((1 + 5)/(1+1))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the TFIDF score**, depend on how we compute the `idf_values`, the `tfidf` is defined by\n",
    "\n",
    "                                tf-idf(w, d) = tf(w, d) * idf(w)\n",
    "\n",
    "Recall that; the meaning of `TF` is **`term frequency`** and here defined by *the number of times that word `w` occurs in document `d`*\n",
    "\n",
    "For example; in the first sentence, `d = 1`; the word `\"and\"` is not in this sentence, so `tf(\"and\", d=1) = 0`.\n",
    "\n",
    "See the table bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea_names</th>\n",
       "      <th>idf_smooth_False)</th>\n",
       "      <th>idf_smooth_True</th>\n",
       "      <th>tfidf_smooth_True_1st_doc</th>\n",
       "      <th>tfidf_smooth_True_2nd_doc</th>\n",
       "      <th>tfidf_smooth_True_3rd_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.182322</td>\n",
       "      <td>0.427120</td>\n",
       "      <td>0.646126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.611659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361255</td>\n",
       "      <td>0.273244</td>\n",
       "      <td>0.245363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>second</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573434</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.182322</td>\n",
       "      <td>0.427120</td>\n",
       "      <td>0.323063</td>\n",
       "      <td>0.290099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>third</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361255</td>\n",
       "      <td>0.273244</td>\n",
       "      <td>0.245363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yours</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fea_names  idf_smooth_False)  idf_smooth_True  tfidf_smooth_True_1st_doc  \\\n",
       "0        and           2.609438         2.098612                   0.000000   \n",
       "1   document           1.223144         1.182322                   0.427120   \n",
       "2      first           1.916291         1.693147                   0.611659   \n",
       "3         is           1.000000         1.000000                   0.361255   \n",
       "4        not           2.609438         2.098612                   0.000000   \n",
       "5        one           2.609438         2.098612                   0.000000   \n",
       "6     second           2.609438         2.098612                   0.000000   \n",
       "7        the           1.223144         1.182322                   0.427120   \n",
       "8      third           2.609438         2.098612                   0.000000   \n",
       "9       this           1.000000         1.000000                   0.361255   \n",
       "10     yours           2.609438         2.098612                   0.000000   \n",
       "\n",
       "    tfidf_smooth_True_2nd_doc  tfidf_smooth_True_3rd_doc  \n",
       "0                    0.000000                   0.514923  \n",
       "1                    0.646126                   0.000000  \n",
       "2                    0.000000                   0.000000  \n",
       "3                    0.273244                   0.245363  \n",
       "4                    0.000000                   0.000000  \n",
       "5                    0.000000                   0.514923  \n",
       "6                    0.573434                   0.000000  \n",
       "7                    0.323063                   0.290099  \n",
       "8                    0.000000                   0.514923  \n",
       "9                    0.273244                   0.245363  \n",
       "10                   0.000000                   0.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = cvect.get_feature_names()\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary = vocabulary, \n",
    "                                           min_df=2, max_df=0.5, ngram_range=(1,2))),\n",
    "                 ('tfid', TfidfTransformer(smooth_idf=True, use_idf = True))]).fit(corpus)\n",
    "\n",
    "count_vector = pipe['count'].transform(corpus).toarray()  ## equivalent with CountVectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_idf_vector = pipe['tfid'].transform(count_vector)\n",
    "tf_idf_vector[0].toarray()\n",
    "table[\"tfidf_smooth_True_1st_doc\"] = tf_idf_vector[0].T.toarray()\n",
    "table[\"tfidf_smooth_True_2nd_doc\"] = tf_idf_vector[1].T.toarray()\n",
    "table[\"tfidf_smooth_True_3rd_doc\"] = tf_idf_vector[2].T.toarray()\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. `TfidfVectorizer` is equivalent to the first method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'not', 'one', 'second', 'the', 'third', 'this', 'yours']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tfidf_matrix =  tf.fit_transform(corpus)\n",
    "feature_names = tf.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Viewing the `tfidf-score` by using `TfidfVectorizer`; first looking at the `tfidf-values` in the first sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.42712001, 0.6116585 , 0.36125537, 0.        ,\n",
       "       0.        , 0.        , 0.42712001, 0.        , 0.36125537,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(corpus)\n",
    "M = tfidf_vectorizer_vectors.toarray()\n",
    "M[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf-idf values using Tfidfvectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea_names</th>\n",
       "      <th>tfidf_TfVec_1st_doc</th>\n",
       "      <th>tfidf_TfVec_2nd_doc</th>\n",
       "      <th>tfidf_TfVec_3rd_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>0.427120</td>\n",
       "      <td>0.646126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>0.611659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>0.361255</td>\n",
       "      <td>0.273244</td>\n",
       "      <td>0.245363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>second</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573434</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>0.427120</td>\n",
       "      <td>0.323063</td>\n",
       "      <td>0.290099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>third</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>0.361255</td>\n",
       "      <td>0.273244</td>\n",
       "      <td>0.245363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yours</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fea_names  tfidf_TfVec_1st_doc  tfidf_TfVec_2nd_doc  tfidf_TfVec_3rd_doc\n",
       "0        and             0.000000             0.000000             0.514923\n",
       "1   document             0.427120             0.646126             0.000000\n",
       "2      first             0.611659             0.000000             0.000000\n",
       "3         is             0.361255             0.273244             0.245363\n",
       "4        not             0.000000             0.000000             0.000000\n",
       "5        one             0.000000             0.000000             0.514923\n",
       "6     second             0.000000             0.573434             0.000000\n",
       "7        the             0.427120             0.323063             0.290099\n",
       "8      third             0.000000             0.000000             0.514923\n",
       "9       this             0.361255             0.273244             0.245363\n",
       "10     yours             0.000000             0.000000             0.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"fea_names\": feature_names, \n",
    "              \"tfidf_TfVec_1st_doc\": M[0, :], \n",
    "              \"tfidf_TfVec_2nd_doc\": M[1, :],\n",
    "              \"tfidf_TfVec_3rd_doc\": M[2, :]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a summary, the main difference between the two modules are as follows:\n",
    "\n",
    "- With `Tfidftransformer` you will systematically compute word counts using `CountVectorizer` and then compute the `Inverse Document Frequency (IDF)` values and only then compute the `Tf-idf scores`.\n",
    "\n",
    "- With `Tfidfvectorizer` on the contrary, **you will do all three steps at once**. It computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\n",
    "\n",
    "**When to use what?**\n",
    "So now you may be wondering, why you should use more steps than necessary if you can get everything done in two steps. Well, there are cases where you want to use Tfidftransformer over Tfidfvectorizer and it is sometimes not that obvious. Here is a general guideline:\n",
    "\n",
    "- If you need the `term frequency` (term count) vectors for `different tasks`, use `Tfidftransformer`.\n",
    "- If you need to compute `tf-idf scores` on documents within your `“training”` dataset, use `Tfidfvectorizer`.\n",
    "- If you need to compute `tf-idf scores` on documents **`outside your “training”`** dataset, use either one, both will work.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `disadvantage` of both preceding methods is that it **only focuses on the frequency of occurrence of a word, leading to it having almost no contextual meaning**. Using **`co-occurrence matrix`** can solves that problem partially. \n",
    "\n",
    "------------------------\n",
    "\n",
    "### 1.3. Co-occurrence Matrix.\n",
    "\n",
    "------------------------\n",
    "\n",
    "**Definition.** `Co-occurrence matrix`, is a symmetric square matrix, each row or column will be the vector representing the corresponding word; it is measuring `co-occurrences` of `features` within a `user-defined context`. \n",
    "\n",
    "The `context` can be defined as a document or a `window` within a collection of documents, with an `optional vector of weights` applied to the co-occurrence counts.\n",
    "\n",
    "------------------------\n",
    "\n",
    "Hence, **`Co-occurrence Matrix`** has the ***advantage of preserving the semantic relationship between words, built on the number of occurrences of word pairs in the `Context Window`***. \n",
    "\n",
    "\n",
    "`A Context Window` is determined by its size and direction, the following table is an example of the `Context Window` with `size = 1`\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bigram\n",
    "\n",
    "https://en.wikipedia.org/wiki/Lexical_analysis#Token\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
    "\n",
    "https://www.sketchengine.eu/user-guide/n-grams/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worried</th>\n",
       "      <th>just</th>\n",
       "      <th>hers</th>\n",
       "      <th>I</th>\n",
       "      <th>NLP</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>kidding</th>\n",
       "      <th>Don't</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>love</th>\n",
       "      <th>and</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hers</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematics</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kidding</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Don't</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             worried  just  hers    I  NLP  Mathematics  kidding  Don't  \\\n",
       "worried          0.0   0.0   0.0  0.0  0.0          0.0      0.0    1.0   \n",
       "just             1.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "hers             0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "I                0.0   0.0   0.0  0.0  1.0          0.0      1.0    0.0   \n",
       "NLP              0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Mathematics      0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "kidding          0.0   1.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Don't            0.0   0.0   1.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Learning         0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Machine          0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "love             0.0   0.0   0.0  3.0  0.0          0.0      0.0    0.0   \n",
       "and              0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "you              0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "\n",
       "             Learning  Machine  love  and  you  \n",
       "worried           0.0      0.0   0.0  0.0  0.0  \n",
       "just              0.0      0.0   0.0  0.0  0.0  \n",
       "hers              0.0      0.0   0.0  1.0  0.0  \n",
       "I                 0.0      0.0   0.0  0.0  0.0  \n",
       "NLP               0.0      0.0   1.0  0.0  0.0  \n",
       "Mathematics       0.0      0.0   0.0  1.0  0.0  \n",
       "kidding           0.0      0.0   0.0  0.0  0.0  \n",
       "Don't             0.0      0.0   0.0  0.0  0.0  \n",
       "Learning          0.0      1.0   0.0  0.0  0.0  \n",
       "Machine           0.0      0.0   1.0  0.0  0.0  \n",
       "love              0.0      0.0   0.0  0.0  0.0  \n",
       "and               1.0      0.0   0.0  0.0  1.0  \n",
       "you               0.0      0.0   1.0  0.0  0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ------------------------- hiden code --------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the `window_size = 1`, then using `bi-grams` is a `sensible way` to build this matrix. The `detailed-discussion` on **`Bi-grams` and `N-grams`** will be introduced later in this link:\n",
    "\n",
    "https://github.com/Nhan121/Lectures_notes-teaching-in-VN-/blob/master/Statistics/NLP/N-grams%20NLP.ipynb\n",
    "\n",
    "**Quick reminder: Bigrams**\n",
    "\n",
    "- A `bigram` or `digram` is a sequence of 2 `adjacent elements` from a `string of tokens`, which are typically `letters`, `syllables`, or `words`.\n",
    "\n",
    "-  The `frequency distribution` of every bigram in a string is commonly used for simple `statistical analysis of text` in many `applications`, such as `computational linguistics`, `cryptography`, `speech recognition`.\n",
    "\n",
    "- Bigrams help provide the `conditional probability` of a `token` given the **`preceding token`**, when the relation of the conditional probability is applied:\n",
    "\n",
    "$$ \\mathbb{P}\\left( \\text{token}_{k} \\left\\vert \\text{token}_{k-1}\\right. \\right) = \\dfrac{\\mathbb{P}\\left( \\text{token}_{k}, \\text{token}_{k-1} \\right)}{ \\mathbb{P} \\left( \\text{token}_{k-1} \\right)} $$\n",
    "\n",
    "For example, the sentence `\"The office building was destroyed yesterday\"` contains 5 `bigrams`:\n",
    "        \n",
    "        the office, office building, building was, was destroyed, demolished yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'office'),\n",
       " ('office', 'building'),\n",
       " ('building', 'was'),\n",
       " ('was', 'destroyed'),\n",
       " ('destroyed', 'yesterday')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The office building was destroyed yesterday\"\n",
    "bi_grams = list(bigrams(sentence.split()))\n",
    "bi_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before build-in a function for the `co_occurrence_matrix`; we will use a few classes and packages; Firstly \n",
    "\n",
    "**Insight coding**\n",
    "Use `itertools.chain.from_iterable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x27243ddac88>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft_text = [[\"I\", \"love\", \"you\", \"and\", \"hers\"], \n",
    "              [\"Don't\", \"worried\", \"just\", \"kidding\"], \n",
    "              [\"I\", \"love\", \"NLP\"],\n",
    "              [\"I\", \"love\", \"Machine\", \"Learning\", \"and\", \"Mathematics\"]]\n",
    "\n",
    "z = itertools.chain.from_iterable(draft_text)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**put the `object: itertools.chain` to list, then we get an 1D vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'you', 'and', 'hers', \"Don't\", 'worried', 'just', 'kidding', 'I', 'love', 'NLP', 'I', 'love', 'Machine', 'Learning', 'and', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "data = list(z)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the vocabulary_list, set and indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'you', 'and', 'hers', \"Don't\", 'worried', 'just', 'kidding', 'I', 'love', 'NLP', 'I', 'love', 'Machine', 'Learning', 'and', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "vocab = set(data)\n",
    "vocab = list(data)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 12,\n",
       " 'love': 13,\n",
       " 'you': 2,\n",
       " 'and': 16,\n",
       " 'hers': 4,\n",
       " \"Don't\": 5,\n",
       " 'worried': 6,\n",
       " 'just': 7,\n",
       " 'kidding': 8,\n",
       " 'NLP': 11,\n",
       " 'Machine': 14,\n",
       " 'Learning': 15,\n",
       " 'Mathematics': 17}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This meant the last time the word appear in the whole sentences. \n",
    "\n",
    "For example the last time the word `Mathematics` appears in the data is `18 (index = 17 in python)`.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "**Create bigrams from all words in corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'love'), ('love', 'you'), ('you', 'and'), ('and', 'hers'), ('hers', \"Don't\"), (\"Don't\", 'worried'), ('worried', 'just'), ('just', 'kidding'), ('kidding', 'I'), ('I', 'love'), ('love', 'NLP'), ('NLP', 'I'), ('I', 'love'), ('love', 'Machine'), ('Machine', 'Learning'), ('Learning', 'and'), ('and', 'Mathematics')]\n"
     ]
    }
   ],
   "source": [
    "bi_grams = list(bigrams(data))\n",
    "print(bi_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Count the bigrams***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('I', 'love'), 3),\n",
       " (('love', 'you'), 1),\n",
       " (('you', 'and'), 1),\n",
       " (('and', 'hers'), 1),\n",
       " (('hers', \"Don't\"), 1),\n",
       " ((\"Don't\", 'worried'), 1),\n",
       " (('worried', 'just'), 1),\n",
       " (('just', 'kidding'), 1),\n",
       " (('kidding', 'I'), 1),\n",
       " (('love', 'NLP'), 1),\n",
       " (('NLP', 'I'), 1),\n",
       " (('love', 'Machine'), 1),\n",
       " (('Machine', 'Learning'), 1),\n",
       " (('Learning', 'and'), 1),\n",
       " (('and', 'Mathematics'), 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that; the `bigram_freq` now is the list of the `tupple`. \n",
    "\n",
    "For instance; the first element in this list is the `tupple: (('I', 'love'), 3)` is combined by the `set: ('I', 'love')` and an  `integer: 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, tuple, int)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigram_freq), type(bigram_freq[1]), type(bigram_freq[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW, assigning the `current(word) = bigram[0][1]` and the `previous(preceding word) = bigram[0][0]` for each iterations `bigram in bigram_freq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('I', 'love'), 3) \t\t love \t\t I \t\t 3\n",
      "(('love', 'you'), 1) \t\t you \t\t love \t\t 1\n",
      "(('you', 'and'), 1) \t\t and \t\t you \t\t 1\n",
      "(('and', 'hers'), 1) \t\t hers \t\t and \t\t 1\n",
      "(('hers', \"Don't\"), 1) \t\t Don't \t\t hers \t\t 1\n",
      "((\"Don't\", 'worried'), 1) \t\t worried \t\t Don't \t\t 1\n",
      "(('worried', 'just'), 1) \t\t just \t\t worried \t\t 1\n",
      "(('just', 'kidding'), 1) \t\t kidding \t\t just \t\t 1\n",
      "(('kidding', 'I'), 1) \t\t I \t\t kidding \t\t 1\n",
      "(('love', 'NLP'), 1) \t\t NLP \t\t love \t\t 1\n",
      "(('NLP', 'I'), 1) \t\t I \t\t NLP \t\t 1\n",
      "(('love', 'Machine'), 1) \t\t Machine \t\t love \t\t 1\n",
      "(('Machine', 'Learning'), 1) \t\t Learning \t\t Machine \t\t 1\n",
      "(('Learning', 'and'), 1) \t\t and \t\t Learning \t\t 1\n",
      "(('and', 'Mathematics'), 1) \t\t Mathematics \t\t and \t\t 1\n"
     ]
    }
   ],
   "source": [
    "co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "for bigram in bigram_freq:\n",
    "    current = bigram[0][1]   ## row 1 ([0]); col 2 ([1]) of the first set in the 2D-set\n",
    "    previous = bigram[0][0]  ## row 1, col 1\n",
    "    count = bigram[1]        ## the values in the 2nd set\n",
    "    print(bigram, '\\t\\t', current, \"\\t\\t\", previous, \"\\t\\t\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build-in the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worried</th>\n",
       "      <th>just</th>\n",
       "      <th>hers</th>\n",
       "      <th>I</th>\n",
       "      <th>NLP</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>kidding</th>\n",
       "      <th>Don't</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>love</th>\n",
       "      <th>and</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hers</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematics</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kidding</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Don't</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             worried  just  hers    I  NLP  Mathematics  kidding  Don't  \\\n",
       "worried          0.0   0.0   0.0  0.0  0.0          0.0      0.0    1.0   \n",
       "just             1.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "hers             0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "I                0.0   0.0   0.0  0.0  1.0          0.0      1.0    0.0   \n",
       "NLP              0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Mathematics      0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "kidding          0.0   1.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Don't            0.0   0.0   1.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Learning         0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "Machine          0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "love             0.0   0.0   0.0  3.0  0.0          0.0      0.0    0.0   \n",
       "and              0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "you              0.0   0.0   0.0  0.0  0.0          0.0      0.0    0.0   \n",
       "\n",
       "             Learning  Machine  love  and  you  \n",
       "worried           0.0      0.0   0.0  0.0  0.0  \n",
       "just              0.0      0.0   0.0  0.0  0.0  \n",
       "hers              0.0      0.0   0.0  1.0  0.0  \n",
       "I                 0.0      0.0   0.0  0.0  0.0  \n",
       "NLP               0.0      0.0   1.0  0.0  0.0  \n",
       "Mathematics       0.0      0.0   0.0  1.0  0.0  \n",
       "kidding           0.0      0.0   0.0  0.0  0.0  \n",
       "Don't             0.0      0.0   0.0  0.0  0.0  \n",
       "Learning          0.0      1.0   0.0  0.0  0.0  \n",
       "Machine           0.0      0.0   1.0  0.0  0.0  \n",
       "love              0.0      0.0   0.0  0.0  0.0  \n",
       "and               1.0      0.0   0.0  0.0  1.0  \n",
       "you               0.0      0.0   1.0  0.0  0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    " \n",
    "    # Create bigrams from all words in corpus\n",
    "    bi_grams = list(bigrams(corpus))\n",
    " \n",
    "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    " \n",
    "    # Initialise co-occurrence matrix\n",
    "    # co_occurrence_matrix[current][previous]\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    " \n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "    \n",
    "    # create matrix\n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    " \n",
    "    # return the matrix and the index\n",
    "    return co_occurrence_matrix, vocab_index\n",
    "\n",
    "draft_data = list(itertools.chain.from_iterable(draft_text))\n",
    "matrix, vocab_index = generate_co_occurrence_matrix(draft_data)\n",
    "  \n",
    "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
    "                             columns=vocab_index)\n",
    "data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>Programming</th>\n",
       "      <th>snake</th>\n",
       "      <th>I</th>\n",
       "      <th>hate</th>\n",
       "      <th>.</th>\n",
       "      <th>love</th>\n",
       "      <th>Math</th>\n",
       "      <th>Biology</th>\n",
       "      <th>and</th>\n",
       "      <th>programming</th>\n",
       "      <th>math</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Programming</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snake</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Math</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biology</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cat  dog  Programming  snake    I  hate    .  love  Math  \\\n",
       "cat          0.0  0.0          0.0    0.0  0.0   1.0  0.0   0.0   0.0   \n",
       "dog          0.0  0.0          0.0    0.0  0.0   1.0  0.0   0.0   0.0   \n",
       "Programming  0.0  0.0          0.0    0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "snake        0.0  0.0          0.0    0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "I            0.0  0.0          0.0    0.0  0.0   0.0  3.0   0.0   0.0   \n",
       "hate         0.0  0.0          0.0    0.0  2.0   0.0  0.0   0.0   0.0   \n",
       ".            1.0  0.0          0.0    0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "love         0.0  0.0          0.0    0.0  4.0   0.0  0.0   0.0   0.0   \n",
       "Math         0.0  0.0          0.0    0.0  0.0   0.0  0.0   1.0   0.0   \n",
       "Biology      0.0  0.0          0.0    0.0  0.0   0.0  0.0   1.0   0.0   \n",
       "and          0.0  1.0          1.0    0.0  0.0   0.0  0.0   0.0   1.0   \n",
       "programming  0.0  0.0          0.0    0.0  0.0   0.0  0.0   1.0   0.0   \n",
       "math         0.0  0.0          0.0    0.0  0.0   0.0  0.0   1.0   0.0   \n",
       "\n",
       "             Biology  and  programming  math  \n",
       "cat              0.0  0.0          0.0   0.0  \n",
       "dog              0.0  0.0          0.0   0.0  \n",
       "Programming      0.0  1.0          0.0   0.0  \n",
       "snake            0.0  1.0          0.0   0.0  \n",
       "I                2.0  0.0          0.0   0.0  \n",
       "hate             0.0  0.0          0.0   0.0  \n",
       ".                0.0  0.0          1.0   1.0  \n",
       "love             0.0  0.0          0.0   0.0  \n",
       "Math             0.0  0.0          0.0   0.0  \n",
       "Biology          0.0  1.0          0.0   0.0  \n",
       "and              0.0  0.0          0.0   0.0  \n",
       "programming      0.0  0.0          0.0   0.0  \n",
       "math             0.0  0.0          0.0   0.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [[\"I\", \"love\", \"math\", \".\" , \"I\", \"love\", \"programming\", \".\", \"I\", \"love\", \"Biology\"],\n",
    "         [\"I\", \"love\", \"Math\", \"and\", \"Programming\", \"and\", \"Biology\"],\n",
    "         [\"I\", \"hate\", \"cat\", \".\" , \"I\", \"hate\", \"dog\", \"and\", \"snake\"]]\n",
    "\n",
    "# Create one list using many lists\n",
    "data = list(itertools.chain.from_iterable(text))\n",
    "matrix, vocab_index = generate_co_occurrence_matrix(data)\n",
    "  \n",
    "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
    "                             columns=vocab_index)\n",
    "data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here the word `‘love’` is defined by the words `‘I’` and `‘Programming’`, meaning that we increment the value both for the `‘I love’` and the `‘love Programming’` co-occurrence. We do that for each window and obtain the preceding `co-occurrence matrix`.\n",
    "\n",
    "- Since `‘Programming’` and `‘Math’` share the same co-occurrence values, they would be placed in the same place; meaning that in this context they mean the same thing (or `‘pretty much’` the same thing). `‘Biology’` would be the closest word to these 2 meaning ‘it has the closest possible meaning but it’s not the same thing’, and so on for every word. The semantic and syntactic relationships generated by this technique are really powerful but it’s computationally expensive since we are talking about a very high-dimensional space. Therefore, we need a technique that reduces dimensionality for us with the least data-loss possible.\n",
    "\n",
    "----------------------------------\n",
    "### Summarizations.\n",
    "\n",
    "#### Advantages\n",
    "- It preserves the `semantic relationship` between words.\n",
    "- It uses `SVD (singular value decomposition)` at its core to reduce the size of vector, which produces more accurate word vector representations than existing methods.\n",
    "- It uses `factorization` which is a `well-defined problem` and can be efficiently solved.\n",
    "- It has to be computed once and can be used anytime once computed. In this sense, it is faster in comparison to others.\n",
    "\n",
    "#### Disadvantage\n",
    "The disadvantage of **`Co-occurrence Matrix (CM)`** is when the `text_data` contains a `large numbers of vocalbularies`; hence it requires huge memory to store the co-occurrence matrix. \n",
    "\n",
    "To make the representation of words clearer and save memory used to store **`CM`**; we have to choose or to remove some unnecessary words (such as **`stopwords`**).\n",
    "\n",
    "--------------------------\n",
    "\n",
    "## 2. Word Embedding: using Prediction Base method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glove Embedding**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
